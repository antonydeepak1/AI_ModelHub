# -*- coding: utf-8 -*-
"""AI COMEDY GENERATOR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19h8oWHGQ_27VDicqkjxiTf1tqNnkAqAR
"""

# STEP 1: Install Dependencies
!pip install -q fastapi uvicorn nest_asyncio transformers accelerate bitsandbytes pyngrok pydantic google-colab

# STEP 2: Import Libraries
import torch
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import nest_asyncio
import uvicorn
from pyngrok import ngrok, conf
import threading
import getpass

# Enable async support for Colab
nest_asyncio.apply()

# STEP 3: Load Comedy Generator Model
print("üîß Loading Comedy Generator model...")
model_name = "teknium/OpenHermes-2.5-Mistral-7B"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
)

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
mistral_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
    torch_dtype=torch.float16,
)

print("‚úÖ Comedy model loaded successfully!")

# STEP 4: Define FastAPI app
app = FastAPI(
    title="AI Comedy & Pickup Line Generator",
    description="Generate hilarious jokes or clever pick-up lines using OpenHermes-2.5 Mistral 7B",
    version="1.0"
)

# STEP 5: Comedy Generator Logic
def generate_comedy(text: str, max_length: int = 100, temperature: float = 0.9, mode: str = "comedy"):
    if mode == "pickup":
        prompt = (
            "You're smooth, clever, and charming. Drop an irresistibly funny and clever pick-up line "
            f"about or related to: {text.strip()}\nPick-up Line:"
        )
    else:  # Default to stand-up comedy
        prompt = (
            "You're a hilarious stand-up comedian performing on stage. "
            "Make the audience laugh out loud with a joke or short routine. "
            f"Topic: {text.strip()}\nComedian:"
        )

    inputs = tokenizer(prompt, return_tensors="pt").to(mistral_model.device)
    outputs = mistral_model.generate(
        **inputs,
        max_new_tokens=max_length,
        temperature=temperature,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        eos_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "").strip()

# STEP 6: Define Request Body
class ComedyRequest(BaseModel):
    text: str
    max_length: int = 100
    temperature: float = 0.9
    mode: str = "comedy"  # "comedy" or "pickup"

# STEP 7: API Endpoints
@app.get("/")
def read_root():
    return {"message": "Welcome to the Comedy & Pickup Line Generator API. Use /docs to explore endpoints."}

@app.post("/generate_comedy/")
async def text_to_comedy(request: ComedyRequest):
    try:
        result = generate_comedy(
            text=request.text,
            max_length=request.max_length,
            temperature=request.temperature,
            mode=request.mode
        )
        return {"result": result}
    except Exception as e:
        return {"error": str(e)}

# STEP 8: Start Server with Ngrok
def start_ngrok():
    NGROK_AUTH_TOKEN = getpass.getpass("üîê Enter your Ngrok Authtoken: ")
    conf.get_default().auth_token = NGROK_AUTH_TOKEN
    public_url = ngrok.connect(8000, "http")
    print(f"\nüåç Public Ngrok URL: {public_url}")
    print(f"üìò Swagger Docs:     {public_url}/docs")

def start_server():
    uvicorn.run(app, host="0.0.0.0", port=8000)

# Start both in parallel
threading.Thread(target=start_server).start()
start_ngrok()

