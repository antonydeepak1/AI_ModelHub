# -*- coding: utf-8 -*-
"""AI COMEDY GENERATOR +ATS SCORE CHECKER+COVER LETTER GENERATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zpLJWivoF-S-o7fKuPU3aDyqAzCfNG5G
"""

# STEP 1: Install dependencies
!pip install -q sentence-transformers pdfplumber pytesseract pillow fastapi uvicorn nest_asyncio transformers accelerate bitsandbytes pyngrok pydantic google-colab opencv-python-headless
!apt-get install -y tesseract-ocr

# STEP 2: Imports
import torch
import pdfplumber
import pytesseract
from PIL import Image
import re
import tempfile
from fastapi import FastAPI, UploadFile, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import nest_asyncio
import uvicorn
from pyngrok import ngrok, conf
import threading
import getpass
from google.colab.output import eval_js

# STEP 3: Enable asyncio in Colab
nest_asyncio.apply()

# STEP 4: Load Resume Analyzer Model
print("üîß Loading Resume Analyzer model...")
embedder = SentenceTransformer("BAAI/bge-small-en", device="cuda" if torch.cuda.is_available() else "cpu")
print("‚úÖ Resume model loaded!")

# STEP 5: Load Comedy Generator Model
print("üîß Loading Comedy Generator model...")
model_name = "teknium/OpenHermes-2.5-Mistral-7B"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
)
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
mistral_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
    torch_dtype=torch.float16,
)
print("‚úÖ Comedy model loaded!")

# STEP 6: Create FastAPI app
app = FastAPI(
    title="Multimodel AI API",
    description="Resume Analyzer + Comedy Generator (Stand-up & Pick-up Lines)",
    version="1.0"
)

# Add CORS support
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Resume Analyzer Utilities ---
def clean_text(text):
    return ' '.join(text.strip().split())

def extract_text_from_pdf(path):
    with pdfplumber.open(path) as pdf:
        return clean_text("\n".join(page.extract_text() or "" for page in pdf.pages))

def extract_text_from_image(path):
    image = Image.open(path)
    return clean_text(pytesseract.image_to_string(image))

def extract_name(text):
    for line in text.split("\n")[:5]:
        if re.match(r"^[A-Z][a-z]+ [A-Z][a-z]+", line.strip()):
            return line.strip()
    return "[Your Name]"

def extract_job_info(jd):
    job_title = re.search(r"(Role|Title|Position)[:\-]?\s*(.*)", jd, re.IGNORECASE)
    company = re.search(r"Company[:\-]?\s*(.*)", jd, re.IGNORECASE)
    return (
        job_title.group(2).strip() if job_title else "the position",
        company.group(1).strip() if company else "your company"
    )

def compute_ats_score(resume, jd):
    emb_resume = embedder.encode(clean_text(resume), convert_to_tensor=True)
    emb_jd = embedder.encode(clean_text(jd), convert_to_tensor=True)
    return round(util.pytorch_cos_sim(emb_resume, emb_jd).item() * 100, 2)

def generate_cover_letter(name, title, company):
    return f"""Dear Hiring Manager,

I am interested in the {title} position at {company}. I believe my skills and experience make me a strong candidate. I am excited about the opportunity to contribute and grow with your team.

Thank you for considering my application.

Sincerely,
{name}
"""

# --- Comedy Generator Function with Mode Support ---
def generate_comedy(text: str, max_length: int = 100, temperature: float = 0.9, mode: str = "comedy"):
    if mode == "pickup":
        prompt = f"Create a funny pick-up line about: {text.strip()}\nPick-up Line:"
    else:  # Default to stand-up comedy
        prompt = f"Create a stand-up comedy joke about: {text.strip()}\nComedian:"

    inputs = tokenizer(prompt, return_tensors="pt").to(mistral_model.device)
    outputs = mistral_model.generate(
        **inputs,
        max_new_tokens=max_length,
        temperature=temperature,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        eos_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "").strip()

# --- Request Model for Comedy Generator ---
class ComedyRequest(BaseModel):
    text: str
    max_length: int = 100
    temperature: float = 0.9
    mode: str = "comedy"  # "comedy" or "pickup"

# --- API Endpoints ---
@app.post("/analyze/")
async def analyze(resume_file: UploadFile, job_desc: str = Form(...), user_name: str = Form("")):
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp.write(await resume_file.read())
        tmp_path = tmp.name

    if resume_file.filename.endswith(".pdf"):
        resume_text = extract_text_from_pdf(tmp_path)
    elif resume_file.filename.lower().endswith((".png", ".jpg", ".jpeg")):
        resume_text = extract_text_from_image(tmp_path)
    else:
        return JSONResponse(content={"error": "Unsupported file type"}, status_code=400)

    name = user_name if user_name.strip() else extract_name(resume_text)
    job_title, company = extract_job_info(job_desc)
    ats_score = compute_ats_score(resume_text, job_desc)
    letter = generate_cover_letter(name, job_title, company)

    return {
        "ats_score": f"{ats_score}%",
        "cover_letter": letter
    }

@app.post("/generate_comedy/")
async def text_to_comedy(request: ComedyRequest):
    result = generate_comedy(request.text, request.max_length, request.temperature, request.mode)
    return {"result": result}

# --- Start Server with Ngrok ---
def start_ngrok():
    NGROK_AUTH_TOKEN = input("üîë Enter your Ngrok Authtoken: ")
    conf.get_default().auth_token = NGROK_AUTH_TOKEN
    public_url = ngrok.connect(8000, "http")
    print(f"\nüåê Public URL: {public_url}")
    print(f"üìò Swagger Docs: {public_url}/docs")

# Start the server and Ngrok tunnel
threading.Thread(target=lambda: uvicorn.run(app, host="0.0.0.0", port=8000)).start()
start_ngrok()

!pip install python-multipart

